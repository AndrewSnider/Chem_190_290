{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9: Neural Networks\n",
    "\n",
    "In this lab, we will explore how to construct a standard feed-forward neural network using only basic numpy and python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Some good materials for understanding neural networks\n",
    "[Neural Networks, Manifolds, and Topology - Colah's blog](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)<br>\n",
    "[How the backpropagation algorithm works - Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html\n",
    ")\n",
    "\n",
    "## The structure and mathematical forms of neural networks\n",
    "![Neural network](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/example_network.svg)\n",
    "<br><br><br>\n",
    "## About activation functions\n",
    "### tanh\n",
    "$y=\\tanh(x)$\n",
    "<br>$y\\in(-1,1)$\n",
    "<br>$y'=1-y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-5,5,2000)\n",
    "y=np.tanh(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid\n",
    "$y={\\displaystyle \\frac{1}{1+e^{-x}} }$\n",
    "<br><br>$y\\in(0,1)$\n",
    "<br>$y'=y(1-y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-5,5,2000)\n",
    "y=1/(1+np.exp(-x))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "$y={\\displaystyle \\begin{equation}\n",
    "\\begin{cases} &x \\:\\:\\: x\\geqslant0 \\\\\n",
    "& 0 \\:\\:\\: x<0\n",
    "\\end{cases}\n",
    "\\end{equation}}$\n",
    "<br><br>$y\\in[0,\\infty)$\n",
    "<br><br>$y'=\\begin{equation}\n",
    "\\begin{cases}\n",
    "& 1\\:\\:\\: x\\geqslant0 \\\\\n",
    "& 0\\:\\:\\: x<0\n",
    "\\end{cases}\n",
    "\\end{equation}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-5,5,2000)\n",
    "y=x*(x>=0)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax\n",
    "$y_i=f_i(\\vec{x})={\\displaystyle \\frac{e^{x_i}}{\\sum_{j=1}^J e^{x_j}}}$\n",
    "<br>$y_i\\in[0,1]$\n",
    "<br><br>${\\displaystyle \\frac{\\partial y_i}{\\partial x_j}=y_i(\\delta_{ij}-y_j)}$\n",
    "\n",
    "<br><br><br><br>\n",
    "## Understanding back propagation\n",
    "http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "$$\\begin{eqnarray} C_{MSE}&=&\\frac{1}{2n}\\lVert a^L-y^{GT}\\rVert^2 \\\\\n",
    "\\delta^L=\\frac{\\partial C}{\\partial z^L}&=&\\nabla_aC\\odot\\sigma'(z^L) \\\\\n",
    "\\delta^l=\\frac{\\partial C}{\\partial z^l}&=&((w^{l+1})^T\\delta^{l+1})\\odot\\sigma'(z^l) \\\\\n",
    "\\frac{\\partial C}{\\partial b^l}&=&\\delta^l \\\\\n",
    "\\frac{\\partial C}{\\partial w^l}&=&(\\delta^l)^T a^{l-1}\n",
    "\\end{eqnarray}$$\n",
    "<br>\n",
    "## Question 1: Implementation of NN in a class\n",
    "\n",
    "Finish the `init_weights` method below by adding in randomly sampled weights and biases using `np.random.random`. Your weights in a given layer should be constructed as a 2d array, your biases as a 1d array. What should the dimensions for your weight array look like based on the architechture of the two layers it connects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN():\n",
    "    def __init__(self,architecture,learning_rate=0.01,activation=lambda x:x):\n",
    "        '''The architecture is a list, with each element specifying the number of nodes in each layer'''\n",
    "        self.arch=architecture\n",
    "        self.activation=activation\n",
    "        self.lr=learning_rate\n",
    "        self.ffcount=len(self.arch)-1\n",
    "        self.initialized=False\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        for n in range(self.ffcount):\n",
    "            prev_layer_neurons=self.arch[n]\n",
    "            current_layer_neurons=self.arch[n+1]\n",
    "            self.weights.append(...)\n",
    "            self.biases.append(...)\n",
    "        self.initialized=True\n",
    "        \n",
    "    def feed_forward(self,x):\n",
    "        if self.initialized:\n",
    "            a_n=np.array(x)\n",
    "            z_s=[]\n",
    "            a_s=[a_n]\n",
    "            for n in range(self.ffcount):\n",
    "                z_n=self.weights[n].dot(a_n)+self.biases[n]\n",
    "                a_n=self.activation(z_n)\n",
    "                z_s.append(z_n)\n",
    "                a_s.append(a_n)\n",
    "            self.z_s=z_s\n",
    "            self.a_s=a_s\n",
    "            self.pred=a_n\n",
    "            return a_n\n",
    "        else:\n",
    "            print(\"Please initialize the weights first!\")\n",
    "            \n",
    "    def calc_error(self,y,activation_grad):\n",
    "        y=np.array(y)\n",
    "        errors=[(self.pred-y)*activation_grad(self.z_s[-1])]\n",
    "        for n in range(self.ffcount-1):\n",
    "            errors.append(self.weights[-n-1].T.dot(errors[-1])*activation_grad(self.z_s[-n-2]))\n",
    "        errors.reverse()\n",
    "        self.errors=errors\n",
    "        return errors\n",
    "            \n",
    "    def calc_grad(self):\n",
    "        weights_grad=[]\n",
    "        for n in range(self.ffcount):\n",
    "            weights_grad.append(self.errors[n].reshape(-1,1).dot(self.a_s[n].reshape(1,-1)))\n",
    "        biases_grad=self.errors\n",
    "        self.weights_grad=weights_grad\n",
    "        self.bias_grad=biases_grad\n",
    "        return weights_grad,biases_grad\n",
    "    \n",
    "    def back_prop(self):\n",
    "        for n in range(self.ffcount):\n",
    "            self.weights[n]=self.weights[n]-self.lr*self.weights_grad[n]\n",
    "            self.biases[n]=self.biases[n]-self.lr*self.bias_grad[n]\n",
    "            \n",
    "    def fit(self,x,y,activation_grad):\n",
    "        self.feed_forward(x)\n",
    "        self.calc_error(y,activation_grad)\n",
    "        self.calc_grad()\n",
    "        self.back_prop()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Activation Functions\n",
    "\n",
    "Implement the tanh activation function and it's respective derivative as setup below so that we can use them in our NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return \n",
    "\n",
    "def tanh_grad(x):\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Work\n",
    "The cell below should run without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NN([6,2,2],activation=tanh)\n",
    "nn.init_weights()\n",
    "print(\"Initialized prediction:\",nn.predict([-1,1,-1,-1,1,-1]))\n",
    "nn.fit([-1,1,-1,-1,1,-1],[-1,-1],tanh_grad)\n",
    "print(\"Error in nodes:\",nn.errors)\n",
    "print(\"Prediction after fitting once:\",nn.predict([-1,1,-1,-1,1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Regularization, L1 and L2\n",
    "<br>\n",
    "$$C=\\frac{1}{2n}\\lVert a^L-y^{GT}\\rVert^2+\\lambda\\sum\\lVert \\theta_i\\rVert ^p$$\n",
    "\n",
    "Copy your working `NN()` class code from the previous question (apart from the __init__ section )and use it as a starting point for the `NN_reg()` class setup below. Your goal will be to modify the `calc_error()` method such that the 'error' variable in that method includeds either L1 (Lasso) or L2 (Regression) depending on user input.\n",
    "\n",
    "**Hint: You can use the .flatten() method on an array of weights in a given layer to get everything into a 1d array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_reg():\n",
    "    def __init__(self,architecture,regularization=None,learning_rate=0.01,activation=lambda x:x,reg_lambda=0.1):\n",
    "        '''The architecture is a list, with each element specifying the number of nodes in each layer'''\n",
    "        self.arch=architecture\n",
    "        self.activation=activation\n",
    "        self.lr=learning_rate\n",
    "        self.ffcount=len(self.arch)-1\n",
    "        self.regularization=regularization\n",
    "        self.reg_lambda=reg_lambda\n",
    "        self.initialized=False\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Testing The Fitting Proccess\n",
    "\n",
    "Using your modified `NN_reg()` code and the one off training example above, create a new model and train it using the same single data point for 1000 epochs. Save the model predictions and relative errors at each epoch, plotting the results in a way that demonstrates the progressive improvement of the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NN_reg([6,2,2],'L2',activation=tanh)\n",
    "nn.init_weights()\n",
    "print(\"Initialized prediction:\",nn.predict([-1,1,-1,-1,1,-1]))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Some guidelines for designing a reasonable NN\n",
    "* The first hidden layer should usually expand the dimension\n",
    "* The following hidden layers should gradually contract (form funnel shape network)\n",
    "* Dimension change between layers should not be crazy\n",
    "* Notice the total number of parameters - ideally less than 1/10 of number of training examples\n",
    "* Should start from shallow networks, if both training error/ test error are high, then consider increasing network complexity\n",
    "* Usually suggested to use ReLU activation for hidden layers (can also try tanh)\n",
    "* Choose the right type of activation for the output layer!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
